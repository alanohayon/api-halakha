import logging
import tempfile
from fastapi import APIRouter, Depends, File, HTTPException, BackgroundTasks, UploadFile, status, Form
from typing import List, Dict, Any, Optional
import uuid
from supabase import Client
import os
from datetime import datetime

# Imports core
from app.core.config import Settings, get_settings
from app.core.database import get_supabase

# Imports services
from app.services.processing_service import ProcessingService
from app.services.supabase_service import SupabaseService

# Imports schemas
from app.schemas.halakha import (
    HalakhaInputBrut,
    HalakhaNotionPost,
)

# Imports utilitaires
from app.utils.validators import sanitize_json_text

# Imports schemas additionnels
from app.schemas.halakha import HalakhaNotionPost

# ============================================================================
# D√âPENDANCES OPTIMIS√âES POUR LA PRODUCTION
# ============================================================================

def get_supabase_service() -> SupabaseService:
    """D√©pendance pour SupabaseService optimis√©e"""
    return SupabaseService()

def get_processing_service() -> ProcessingService:
    """D√©pendance pour ProcessingService optimis√©e"""
    return ProcessingService()

router = APIRouter()
logger = logging.getLogger(__name__)

@router.post("/halakhot/post", response_model=HalakhaNotionPost)
async def process_halakha_to_notion(
    content: str = Form(..., min_length=10, max_length=10000, description="Contenu de la halakha √† traiter"),
    schedule_days: int = Form(
        default=0, 
        ge=0, 
        le=100, 
        description="Nombre de jours de report pour la programmation (0-100 uniquement)",
        example=7
    ),
    last_img: bool = Form(
        default=False,
        description="Si True, sauvegarde la derni√®re image dans Supabase puis dans notion"
    ),
    processing_service: ProcessingService = Depends(get_processing_service)
):
    """
    Traite une halakha avec OpenAI et la publie sur Notion.
    
    Args:
        content: Le texte complet de la halakha √† analyser
        schedule_days: Nombre de jours √† ajouter pour la date de publication (0-100)
        
    Returns:
        HalakhaNotionPost: R√©ponse contenant l'URL de la page Notion cr√©√©e
        
    Raises:
        HTTPException: Si erreur lors du traitement
    """
    logger.info(f"üîÑ Requ√™te re√ßue pour traiter une halakha vers Notion")
    
    # Validation stricte du contenu
    if not content or not content.strip():
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail="Le contenu de la halakha ne peut pas √™tre vide"
        )
    
    if schedule_days < 0 or schedule_days > 100:
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail="schedule_days doit √™tre entre 0 et 100 inclus"
        )
    
    logger.info(f"‚úÖ Param√®tres valid√©s - Contenu: {len(content.strip())} caract√®res, Jours: {schedule_days}")
    
    try:
        # Nettoyer le contenu textuel
        sanitized_content = sanitize_json_text(content.strip())
        
        # Lancer le processus complet via ProcessingService (avec sauvegarde Supabase)
        notion_url = await processing_service.process_halakha_complete(
            halakha_content=sanitized_content,
            add_day_for_notion=schedule_days,
            last_image=last_img  # Save la derniere image dans supabase puis dans notion
        )
        
        logger.info(f"‚úÖ Halakha trait√©e avec succ√®s. URL Notion: {notion_url}")
        
        return HalakhaNotionPost(notion_page_url=notion_url)

    except Exception as e:
        logger.error(f"‚ùå Erreur lors du traitement halakha vers Notion: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Une erreur interne est survenue: {str(e)}"
        )
        
        
@router.post("/batch/halakhot", status_code=status.HTTP_201_CREATED)
async def process_halakhot_from_json(
    start_index: int = Form(
        default=0, 
        ge=0, 
        description="Index de d√©part dans le fichier JSON des halakhot (commence √† 0)",
        example=0
    ),
    limit_halakhot: int = Form(
        default=10, 
        ge=1, 
        le=50, 
        description="Nombre maximum d'halakhot √† traiter (1-50)",
        example=10
    ),
    schedule_days: int = Form(
        default=0, 
        ge=0, 
        le=365, 
        description="Nombre de jours de d√©calage pour la premi√®re halakha (0-365)",
        example=0
    ),
    max_retries: int = Form(
        default=3, 
        ge=0, 
        le=10, 
        description="Nombre maximum de tentatives par halakha (0-10)",
        example=3
    ),
    fail_fast_on_max_retries: bool = Form(
        default=True, 
        description="Si True, arr√™te le batch en cas d'√©chec d√©finitif d'une halakha"
    ),
    processing_service: ProcessingService = Depends(get_processing_service)
):
    """
    üöÄ Traite plusieurs halakhot en lot depuis le fichier JSON vers Notion
    
    Args:
        start_index: Index de d√©part dans le fichier JSON (0-based)
        limit_halakhot: Nombre maximum d'halakhot √† traiter (1-50)
        schedule_days: Jours de d√©calage pour la premi√®re halakha (auto-incr√©ment√©)
        max_retries: Nombre maximum de tentatives par halakha
        fail_fast_on_max_retries: Arr√™t du batch si √©chec d√©finitif
        
    Returns:
        Rapport d√©taill√© du traitement en lot avec statistiques
        
    Raises:
        HTTPException: Si erreur de validation ou traitement
    """
    logger.info(f"üöÄ D√©marrage batch processing - Range: {start_index}-{start_index + limit_halakhot - 1}")
    logger.info(f"üìã Param√®tres: schedule_days={schedule_days}, max_retries={max_retries}, fail_fast={fail_fast_on_max_retries}")
    
    # Validation des param√®tres
    if limit_halakhot > 50:
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail="limit_halakhot ne peut pas d√©passer 50 pour √©viter les timeouts"
        )
    
    if schedule_days > 365:
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail="schedule_days ne peut pas d√©passer 365 jours"
        )
    
    try:
        # Lancer le traitement en lot via ProcessingService
        batch_result = await processing_service.process_halakhot_from_json(
            start_index=start_index,
            schedule_days=schedule_days,
            limit_halakhot=limit_halakhot,
            max_retries=max_retries,
            fail_fast_on_max_retries=fail_fast_on_max_retries
        )
    # D√©terminer le statut HTTP selon le r√©sultat
        if batch_result["status"] == "failed_fast":
            # Fail-fast d√©clench√© - retourner 207 Multi-Status
            status_code = status.HTTP_207_MULTI_STATUS
            message = f"Batch arr√™t√© en fail-fast √† la halakha #{batch_result.get('fail_fast_triggered_at_index', 'N/A')}"
        elif batch_result["failed_count"] > 0:
            # Succ√®s partiel - retourner 207 Multi-Status  
            status_code = status.HTTP_207_MULTI_STATUS
            message = f"Batch compl√©t√© avec {batch_result['failed_count']} √©chec(s) sur {batch_result['processed_count']}"
        else:
            # Succ√®s complet - retourner 200 OK
            status_code = status.HTTP_200_OK
            message = f"Batch compl√©t√© avec succ√®s - {batch_result['success_count']} halakhot trait√©es"
        
        logger.info(f"‚úÖ {message}")
        
        # Retourner la r√©ponse enrichie
        return {
            "status": "success" if batch_result["failed_count"] == 0 else "partial_success",
            "message": message,
            "data": batch_result,
            "summary": {
                "total_processed": batch_result["processed_count"],
                "successful": batch_result["success_count"],
                "failed": batch_result["failed_count"],
                "skipped": batch_result.get("skipped_count", 0),
                "success_rate": batch_result["success_rate"],
                "range": f"{batch_result['start_index']}-{batch_result.get('end_index', 'N/A')}",
                "fail_fast_triggered": batch_result["status"] == "failed_fast"
            }
        }
        
    except RuntimeError as e:
        # Erreur fail-fast ou critique
        if "fail-fast" in str(e):
            logger.error(f"üö® Fail-fast d√©clench√©: {e}")
            raise HTTPException(
                status_code=status.HTTP_409_CONFLICT,
                detail=f"Traitement arr√™t√© en mode fail-fast: {str(e)}"
            )
        else:
            logger.error(f"‚ùå Erreur critique du batch: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Erreur critique lors du traitement: {str(e)}"
            )
    
    except ValueError as e:
        # Erreur de validation ou param√®tres invalides
        logger.error(f"‚ùå Erreur de validation: {e}")
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail=f"Param√®tres invalides: {str(e)}"
        )
    
    except Exception as e:
        # Erreur inattendue
        logger.error(f"‚ùå Erreur inattendue lors du batch: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Une erreur inattendue est survenue: {str(e)}"
        )

@router.post("/images", status_code=status.HTTP_201_CREATED)
async def upload_image(
    file: UploadFile = File(..., description="Fichier image √† uploader"),
    clean_filename: Optional[str] = Form(None, description="Nom de fichier personnalis√©")
):
    """
    Upload une image vers Supabase Storage et retourne l'URL publique
    
    Args:
        file: Fichier image (.png, .jpg, .jpeg, .webp) √† uploader
        clean_filename: Nom de fichier personnalis√© (optionnel)
        
    Returns:
        dict: R√©ponse contenant l'URL de l'image upload√©e
        
    Raises:
        HTTPException: Si erreur lors de l'upload ou format de fichier invalide
    """
    logger.info(f"Requ√™te re√ßue pour upload d'image : {file.filename}")
    
    # Validation basique du fichier
    if not file.filename:
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail="Nom de fichier requis"
        )
    
    # V√©rifier le type de fichier
    allowed_extensions = {".png", ".jpg", ".jpeg", ".webp"}
    file_extension = os.path.splitext(file.filename.lower())[1]
    
    if file_extension not in allowed_extensions:
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail=f"Format non support√©. Formats accept√©s : {', '.join(allowed_extensions)}"
        )
    
    # V√©rifier le content-type
    allowed_content_types = {"image/png", "image/jpeg", "image/jpg", "image/webp"}
    if file.content_type not in allowed_content_types:
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail=f"Type MIME non support√© : {file.content_type}"
        )
    
    # V√©rifier la taille du fichier (max 10MB)
    file_content = await file.read()
    if len(file_content) > 10 * 1024 * 1024:  # 10MB
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail="Fichier trop volumineux (max 10MB)"
        )
    
    logger.info(f"Param√®tres valid√©s - Fichier: {file.filename}, Taille: {len(file_content)} bytes")
    
    try:
        # Instancier le service d'orchestration
        processing_service = ProcessingService()
        
        # Lancer l'upload via le service d'orchestration
        result = await processing_service.upload_image_to_storage(
            file_content=file_content,
            filename=file.filename,
            clean_filename=clean_filename
        )
        
        logger.info(f"‚úÖ Image upload√©e avec succ√®s : {result['filename']}")
        
        return {
            "status": "success", 
            "message": "Image upload√©e avec succ√®s",
            "data": result
        }
        
    except Exception as e:
        logger.error(f"Erreur lors de l'upload d'image : {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Une erreur interne est survenue : {str(e)}"
        )

@router.get("/images/latest")
async def get_latest_image():
    """
    R√©cup√®re l'URL de la derni√®re image upload√©e dans Supabase Storage
    
    Returns:
        dict: R√©ponse contenant l'URL de la derni√®re image
        
    Raises:
        HTTPException: Si aucune image trouv√©e ou erreur
    """
    logger.info("Requ√™te re√ßue pour r√©cup√©rer la derni√®re image")
    
    try:
        # Instancier le service d'orchestration 
        processing_service = ProcessingService()
        
        # R√©cup√©rer la derni√®re image via Supabase service
        image_url, name = await processing_service.supabase_service.get_last_img_supabase()
        
        if not image_url:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Aucune image trouv√©e dans le storage "
            )

        
        return {
            "status": "success",
            "message": "Derni√®re image r√©cup√©r√©e avec succ√®s",
            "data": {
                "image_url": image_url,
                "name": name
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration de la derni√®re image : {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Une erreur interne est survenue : {str(e)}"
        )



# # Stockage des jobs en m√©moire (en production, utiliser Redis/DB)
# job_status_store = {}

# class JobStatus:
#     PENDING = "pending"
#     PROCESSING = "processing"
#     COMPLETED = "completed"
#     FAILED = "failed"

# class JobResponse:
#     def __init__(self, job_id: str, status: str, message: str, result: Any = None, error: str = None):
#         self.job_id = job_id
#         self.status = status
#         self.message = message
#         self.result = result
#         self.error = error
#         self.created_at = datetime.now().isoformat()

# def get_openai_service(settings: Settings = Depends(get_settings)) -> OpenAIService:
#     """Dependency pour obtenir le service OpenAI"""
#     return OpenAIService()

# async def process_halakha_background(job_id: str, halakha_content: str, add_day: int = 0):
#     """T√¢che de fond pour traiter une halakha"""
    
#     # Mettre √† jour le statut du job
#     job_status_store[job_id] = JobResponse(
#         job_id=job_id,
#         status=JobStatus.PROCESSING,
#         message="Traitement de la halakha en cours..."
#     ).__dict__
    
#     try:
#         logger.info(f"üöÄ D√©marrage du job {job_id}")
        
#         # Nettoyer le contenu textuel
#         from app.utils.validators import sanitize_json_text
#         halakha_content = sanitize_json_text(halakha_content)
        
#         settings = get_settings()
#         processing_service = ProcessingService()
        
#         # Utiliser le service de traitement pour le processus complet
#         notion_page_url = await processing_service.process_halakha_for_notion(
#             halakha_content=halakha_content,
#             add_day_for_notion=add_day
#         )
        
#         # Succ√®s
#         job_status_store[job_id] = JobResponse(
#             job_id=job_id,
#             status=JobStatus.COMPLETED,
#             message="Halakha trait√©e et page Notion cr√©√©e avec succ√®s",
#             result={"notion_page_url": notion_page_url}
#         ).__dict__
        
#         logger.info(f"‚úÖ Job {job_id} termin√© avec succ√®s")
        
#     except Exception as e:
#         logger.error(f"‚ùå Erreur lors du traitement du job {job_id}: {e}")
        
#         # √âchec
#         job_status_store[job_id] = JobResponse(
#             job_id=job_id,
#             status=JobStatus.FAILED,
#             message="Erreur lors du traitement de la halakha",
#             error=str(e)
#         ).__dict__

# @router.post("/process/start")
# async def start_halakha_processing(
#     background_tasks: BackgroundTasks,
#     halakha_content: str,
#     add_day: int = 0
# ):
#     """
#     D√©marre le traitement d'une halakha en arri√®re-plan
#     Retourne imm√©diatement un job_id pour suivre le progr√®s
#     """
    
#     # G√©n√©rer un ID unique pour le job
#     job_id = str(uuid.uuid4())
    
#     # Initialiser le statut du job
#     job_status_store[job_id] = JobResponse(
#         job_id=job_id,
#         status=JobStatus.PENDING,
#         message="Job en attente de traitement..."
#     ).__dict__
    
#     # Lancer la t√¢che en arri√®re-plan
#     background_tasks.add_task(
#         process_halakha_background,
#         job_id,
#         halakha_content,
#         add_day
#     )
    
#     logger.info(f"üöÄ Job {job_id} cr√©√© et ajout√© √† la queue")
    
#     return {
#         "job_id": job_id,
#         "status": "started",
#         "message": "Traitement d√©marr√© en arri√®re-plan",
#         "polling_url": f"/api/v1/process/status/{job_id}"
#     }

# @router.get("/process/status/{job_id}")
# async def get_job_status(job_id: str):
#     """
#     R√©cup√®re le statut d'un job de traitement
#     Utilis√© pour le polling c√¥t√© client
#     """
    
#     if job_id not in job_status_store:
#         raise HTTPException(
#             status_code=status.HTTP_404_NOT_FOUND,
#             detail=f"Job {job_id} non trouv√©"
#         )
    
#     return job_status_store[job_id]

# @router.get("/process/jobs")
# async def list_all_jobs():
#     """
#     Liste tous les jobs (pour debug/monitoring)
#     """
#     return {
#         "jobs": list(job_status_store.values()),
#         "total": len(job_status_store)
#     }

# @router.delete("/process/jobs/{job_id}")
# async def cancel_job(job_id: str):
#     """
#     Annule/supprime un job
#     Note: L'annulation de t√¢ches en cours n√©cessiterait une logique plus complexe
#     """
    
#     if job_id not in job_status_store:
#         raise HTTPException(
#             status_code=status.HTTP_404_NOT_FOUND,
#             detail=f"Job {job_id} non trouv√©"
#         )
    
#     # Supprimer du store
#     del job_status_store[job_id]
    
#     return {
#         "message": f"Job {job_id} supprim√©",
#         "status": "cancelled"
#     }

# ============== ANCIEN ENDPOINT (D√âPR√âCI√â) ==============

# @router.post("/halakhot", response_model=HalakhaNotionPost)
# async def process_halakha(
#     content: str = Form(..., min_length=10, max_length=10000, description="Contenu de la halakha √† traiter"),
#     schedule_days: int = Form(default=0, ge=0, le=100, description="Nombre de jours de report (0-100)"),
#     processing_service: ProcessingService = Depends(get_processing_service),
#     settings: Settings = Depends(get_settings)
# ):
#     """
#     Traitement complet d'une halakha : analyse OpenAI + sauvegarde Supabase + cr√©ation page Notion
    
#     Args:
#         content: Le texte complet de la halakha √† analyser
#         schedule_days: Nombre de jours √† ajouter pour la date de publication (0-100)
        
#     Returns:
#         HalakhaNotionPost: R√©ponse contenant l'URL de la page Notion cr√©√©e
        
#     Raises:
#         HTTPException: Si erreur lors du traitement
#     """
#     # Validation stricte du contenu
#     if not content or not content.strip():
#         raise HTTPException(
#             status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
#             detail="Le contenu de la halakha ne peut pas √™tre vide"
#         )
    
#     if schedule_days < 0 or schedule_days > 100:
#         raise HTTPException(
#             status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
#             detail="schedule_days doit √™tre entre 0 et 100 inclus"
#         )
    
#     logger.info(f"üöÄ D√©but du traitement - Contenu: {len(content.strip())} chars, Jours: {schedule_days}")
    
#     try:
#         # Nettoyer le contenu textuel
#         sanitized_content = sanitize_json_text(content.strip())
        
#         # Traitement complet via le service d'orchestration
#         notion_url = await processing_service.process_halakha_for_notion(
#             halakha_content=sanitized_content,
#             add_day_for_notion=schedule_days
#         )
        
#         logger.info(f"‚úÖ Traitement termin√© avec succ√®s : {notion_url}")
        
#         return HalakhaNotionPost(notion_page_url=notion_url)
        
#     except Exception as e:
#         logger.error(f"‚ùå Erreur lors du traitement : {e}", exc_info=True)
#         raise HTTPException(
#             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
#             detail=f"Erreur lors du traitement : {str(e)}"
#         )

# ============================================================================
# ENDPOINT DE SANT√â POUR MONITORING PRODUCTION
# ============================================================================

@router.get("/health")
async def health_check(
    settings: Settings = Depends(get_settings)
):
    """
    Endpoint de sant√© pour v√©rifier la configuration des services
    Essentiel pour le monitoring en production
    """
    try:
        health_status = {
            "status": "healthy",
            "timestamp": datetime.utcnow().isoformat(),
            "services": {}
        }
        
        # V√©rification OpenAI
        if settings.openai_api_key:
            health_status["services"]["openai"] = "configured"
        else:
            health_status["services"]["openai"] = "not_configured"
        
        # V√©rification Notion
        if settings.notion_api_token:
            health_status["services"]["notion"] = "configured"
        else:
            health_status["services"]["notion"] = "not_configured"
        
        # V√©rification Supabase
        if settings.supabase_url and settings.supabase_anon_key:
            health_status["services"]["supabase"] = "configured"
        else:
            health_status["services"]["supabase"] = "not_configured"
        
        return health_status
        
    except Exception as e:
        logger.error(f"‚ùå Erreur health check : {e}")
        return {
            "status": "unhealthy",
            "error": str(e)
        }


        
# @router.post("/halakha/analyze", response_model=ProcessHalakhaResponse, tags=["AI Processing"])
# async def analyze_halakha(
#     request: ProcessHalakhaRequest,
#     openai_service: OpenAIService = Depends(get_openai_service)
# ):
#     """
#     Analyser une halakha et extraire les donn√©es structur√©es (question, r√©ponse, sources, etc.)
    
#     Cette route utilise OpenAI pour traiter le contenu d'une halakha et en extraire
#     les informations structur√©es comme la question, la r√©ponse, les sources et les th√®mes.
#     """
#     try:
#         logger.info("D√©but de l'analyse de la halakha...")
        
#         # Nettoyer le contenu textuel
#         sanitized_data = validate_and_sanitize_request(request, ['content'])
#         request.content = sanitized_data['content']
        
#         # Traitement avec OpenAI pour extraire les donn√©es structur√©es
#         processed_data = openai_service.queries_halakha(request.content)
        
#         logger.info("Analyse de la halakha termin√©e avec succ√®s")
        
#         return ProcessHalakhaResponse(
#             success=True,
#             message="Halakha analys√©e avec succ√®s",
#             **processed_data
#         )
        
#     except Exception as e:
#         logger.error(f"Erreur lors de l'analyse de la halakha : {e}")
#         raise HTTPException(
#             status_code=500, 
#             detail=f"Erreur lors de l'analyse de la halakha : {str(e)}"
#         )

# @router.post("/halakha/complete", response_model=ProcessCompleteHalakhaResponse, tags=["AI Processing"])
# async def process_complete_halakha(
#     request: ProcessHalakhaRequest,
#     openai_service: OpenAIService = Depends(get_openai_service)
# ):
#     """
#     Traitement complet d'une halakha : analyse + g√©n√©ration de contenu pour post
    
#     Cette route effectue un traitement complet d'une halakha :
#     1. Analyse et extraction des donn√©es structur√©es (question, r√©ponse, sources, etc.)
#     2. G√©n√©ration du texte pour le post Instagram
#     3. G√©n√©ration de la l√©gende pour le post
    
#     Tout est trait√© et retourn√© dans une seule r√©ponse JSON.
#     """
#     try:
#         logger.info("D√©but du traitement complet de la halakha...")
        
#         # Nettoyer le contenu textuel
#         sanitized_data = validate_and_sanitize_request(request, ['content'])
#         request.content = sanitized_data['content']
        
#         # √âtape 1: Analyser la halakha pour extraire les donn√©es structur√©es
#         logger.info("Analyse de la halakha...")
#         processed_data = openai_service.queries_halakha(request.content)
        
#         # √âtape 2: G√©n√©rer le contenu pour le post et la l√©gende
#         logger.info("G√©n√©ration du contenu pour le post...")
#         text_post, legend = openai_service.queries_post_legende(
#             request.content, 
#             processed_data["answer"]
#         )
        
#         # Combiner toutes les donn√©es
#         complete_data = {
#             **processed_data,
#             "text_post": text_post,
#             "legend": legend
#         }
        
#         logger.info("Traitement complet de la halakha termin√© avec succ√®s")
        
#         return ProcessCompleteHalakhaResponse(
#             success=True,
#             message="Halakha trait√©e compl√®tement avec succ√®s",
#             **complete_data
#         )
        
#     except Exception as e:
#         logger.error(f"Erreur lors du traitement complet de la halakha : {e}")
#         raise HTTPException(
#             status_code=500, 
#             detail=f"Erreur lors du traitement complet de la halakha : {str(e)}"
#         ) 

# @router.post("/halakhot/batch")
# async def process_batch_halakhot(
#     *,
#     background_tasks: BackgroundTasks,
#     supabase: Client = Depends(get_supabase),
#     settings: Settings = Depends(get_settings),
#     halakhot_inputs: List[HalakhaTextInput]
# ):
#     """
#     Traite plusieurs halakhot en arri√®re-plan.
#     Retourne imm√©diatement un ID de job pour suivre le progr√®s.
#     """
#     logger.info(f"Requ√™te re√ßue pour traiter {len(halakhot_inputs)} halakhot en lot.")
    
#     if not halakhot_inputs:
#         raise HTTPException(
#             status_code=400,
#             detail="La liste des halakhot ne peut pas √™tre vide"
#         )
    
#     if len(halakhot_inputs) > 50:  # Limite raisonnable
#         raise HTTPException(
#             status_code=400,
#             detail="Trop de halakhot √† traiter en une fois (maximum 50)"
#         )
    
#     try:
#         # Nettoyer tous les champs textuels dans la liste
#         for i, halakha_input in enumerate(halakhot_inputs):
#             sanitized_data = validate_and_sanitize_request(halakha_input, ['content'])
#             halakhot_inputs[i].content = sanitized_data['content']
        
#         # G√©n√©rer un ID unique pour ce job
#         job_id = str(uuid.uuid4())
        
#         # Initialiser le statut du job
#         job_status_store[job_id] = {
#             "job_id": job_id,
#             "status": "pending",
#             "total": len(halakhot_inputs),
#             "processed": 0,
#             "errors": [],
#             "created_at": "2024-01-01T00:00:00Z",  # En production, utiliser datetime.utcnow()
#             "started_at": None,
#             "completed_at": None
#         }
        
#         # Ajouter la t√¢che en arri√®re-plan
#         background_tasks.add_task(
#             _process_batch_background,
#             job_id=job_id,
#             halakhot_inputs=halakhot_inputs,
#             supabase=supabase,
#             settings=settings
#         )
        
#         return {
#             "status": "accepted",
#             "message": f"Traitement de {len(halakhot_inputs)} halakhot d√©marr√© en arri√®re-plan",
#             "job_id": job_id,
#             "total_items": len(halakhot_inputs),
#             "status_url": f"/api/v1/processing/jobs/{job_id}"
#         }
        
#     except Exception as e:
#         logger.error(f"Erreur lors de l'initialisation du traitement batch: {e}", exc_info=True)
#         raise HTTPException(
#             status_code=500,
#             detail=f"Impossible de d√©marrer le traitement en lot: {str(e)}"
#         )


# @router.get("/jobs/{job_id}")
# async def get_job_status(job_id: str):
#     """
#     R√©cup√®re le statut d'un job de traitement asynchrone.
    
#     Statuts possibles :
#     - pending: Job cr√©√© mais pas encore d√©marr√©
#     - running: Job en cours d'ex√©cution
#     - completed: Job termin√© avec succ√®s
#     - failed: Job √©chou√©
#     - partial: Job termin√© avec quelques erreurs
#     """
#     if job_id not in job_status_store:
#         raise HTTPException(
#             status_code=404,
#             detail="Job not found"
#         )
    
#     job_status = job_status_store[job_id]
    
#     # Calculer le pourcentage de progression
#     if job_status["total"] > 0:
#         progress_percentage = (job_status["processed"] / job_status["total"]) * 100
#     else:
#         progress_percentage = 0
    
#     return {
#         **job_status,
#         "progress_percentage": round(progress_percentage, 2)
#     }

# @router.delete("/jobs/{job_id}")
# async def cancel_job(job_id: str):
#     """
#     Annule un job en cours (si possible).
#     Note: L'annulation n'est pas toujours possible selon l'√©tat du job.
#     """
#     if job_id not in job_status_store:
#         raise HTTPException(
#             status_code=404,
#             detail="Job not found"
#         )
    
#     job_status = job_status_store[job_id]
    
#     if job_status["status"] in ["completed", "failed"]:
#         raise HTTPException(
#             status_code=400,
#             detail=f"Cannot cancel job with status: {job_status['status']}"
#         )
    
#     # Marquer comme annul√©
#     job_status_store[job_id]["status"] = "cancelled"
#     job_status_store[job_id]["completed_at"] = "2024-01-01T00:00:00Z"  # En production, utiliser datetime.utcnow()
    
#     return {
#         "message": "Job cancelled successfully",
#         "job_id": job_id
#     }

# async def _process_batch_background(
#     job_id: str,
#     halakhot_inputs: List[HalakhaTextInput],
#     supabase: Client,
#     settings: Settings
# ):
#     """
#     Fonction priv√©e pour traiter les halakhot en arri√®re-plan.
#     """
#     logger.info(f"D√©marrage du traitement batch {job_id}")
    
#     # Mettre √† jour le statut
#     job_status_store[job_id]["status"] = "running"
#     job_status_store[job_id]["started_at"] = "2024-01-01T00:00:00Z"  # En production, utiliser datetime.utcnow()
    
#     processing_service = ProcessingService(supabase_client=supabase, settings=settings)
    
#     processed = 0
#     errors = []
    
#     for i, halakha_input in enumerate(halakhot_inputs):
#         # V√©rifier si le job a √©t√© annul√©
#         if job_status_store[job_id]["status"] == "cancelled":
#             logger.info(f"Job {job_id} cancelled, stopping processing")
#             break
            
#         try:
#             logger.info(f"Traitement de la halakha {i+1}/{len(halakhot_inputs)} (job {job_id})")
            
#             await processing_service.process_and_publish_halakha(
#                 halakha_content=halakha_input.content,
#                 add_day_for_notion=halakha_input.schedule_days + i  # √âtaler les dates de publication
#             )
            
#             processed += 1
#             logger.info(f"Halakha {i+1} trait√©e avec succ√®s")
            
#             # Mettre √† jour le progr√®s
#             job_status_store[job_id]["processed"] = processed
            
#         except Exception as e:
#             error_msg = f"Erreur lors du traitement de la halakha {i+1}: {str(e)}"
#             logger.error(error_msg, exc_info=True)
#             errors.append({
#                 "halakha_index": i + 1,
#                 "error": error_msg
#             })
#             job_status_store[job_id]["errors"] = errors
    
#     # Finaliser le statut du job
#     if job_status_store[job_id]["status"] != "cancelled":
#         if len(errors) == 0:
#             job_status_store[job_id]["status"] = "completed"
#         elif processed > 0:
#             job_status_store[job_id]["status"] = "partial"  # Succ√®s partiel
#         else:
#             job_status_store[job_id]["status"] = "failed"
    
#     job_status_store[job_id]["completed_at"] = "2024-01-01T00:00:00Z"  # En production, utiliser datetime.utcnow()
    
#     logger.info(f"Traitement job {job_id} termin√©: {processed}/{len(halakhot_inputs)} r√©ussies, {len(errors)} erreurs")

# @router.post("/upload-latest-image")
# async def upload_latest_image(
#     supabase_client=Depends(get_supabase)
# ) -> Dict[str, Any]:
#     """
#     Upload la derni√®re image du dossier Downloads vers Supabase Storage
#     et retourne l'URL publique
#     """
#     try:
#         logger.info("üñºÔ∏è Recherche de la derni√®re image dans Downloads...")
        
#         # R√©cup√©rer la derni√®re image avec nom nettoy√©
#         latest_image_path, clean_filename = get_latest_image_with_clean_name()
        
#         if not latest_image_path or not clean_filename:
#             raise HTTPException(
#                 status_code=status.HTTP_404_NOT_FOUND,
#                 detail="Aucune image trouv√©e dans le dossier Downloads"
#             )
        
#         logger.info(f"üìÅ Image trouv√©e: {latest_image_path}")
        
#         # Upload vers Supabase
#         supabase_service = SupabaseService(supabase_client)
#         image_url = await supabase_service.upload_img_to_supabase(latest_image_path, clean_filename)
        
#         if not image_url:
#             raise HTTPException(
#                 status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
#                 detail="√âchec de l'upload de l'image"
#             )
        
#         logger.info(f"‚úÖ Image upload√©e avec succ√®s: {image_url}")
        
#         return {
#             "success": True,
#             "message": "Image upload√©e avec succ√®s",
#             "data": {
#                 "image_url": image_url,
#                 "file_name": clean_filename,
#                 "original_file_name": latest_image_path.split("/")[-1],
#                 "bucket": "notion-images"
#             }
#         }
        
#     except HTTPException:
#         raise
#     except Exception as e:
#         logger.error(f"‚ùå Erreur lors de l'upload de l'image: {e}")
#         raise HTTPException(
#             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
#             detail=f"Erreur interne: {str(e)}"
#         )

# @router.get("/latest-image-info")
# async def get_latest_image_info() -> Dict[str, Any]:
#     """
#     R√©cup√®re les informations sur la derni√®re image dans Downloads
#     sans l'uploader
#     """
#     try:
#         latest_image_path, clean_filename = get_latest_image_with_clean_name()
        
#         if not latest_image_path or not clean_filename:
#             raise HTTPException(
#                 status_code=status.HTTP_404_NOT_FOUND,
#                 detail="Aucune image trouv√©e dans le dossier Downloads"
#             )
        
#         file_stats = os.stat(latest_image_path)
#         file_size = file_stats.st_size
#         modified_time = datetime.fromtimestamp(file_stats.st_mtime)
        
#         return {
#             "success": True,
#             "data": {
#                 "file_path": latest_image_path,
#                 "original_file_name": os.path.basename(latest_image_path),
#                 "clean_file_name": clean_filename,
#                 "file_size_bytes": file_size,
#                 "file_size_mb": round(file_size / (1024 * 1024), 2),
#                 "modified_time": modified_time.isoformat(),
#                 "ready_for_upload": True
#             }
#         }
        
#     except HTTPException:
#         raise
#     except Exception as e:
#         logger.error(f"‚ùå Erreur lors de la r√©cup√©ration des infos de l'image: {e}")
#         raise HTTPException(
#             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
#             detail=f"Erreur interne: {str(e)}"
#         )

